{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "First, let's load the JSON file which describes the human pose task.  This is in COCO format, it is the category descriptor pulled from the annotations file.  We modify the COCO category slightly, to add a neck keypoint.  We will use this task description JSON to create a topology tensor, which is an intermediate data structure that describes the part linkages, as well as which channels in the part affinity field each linkage corresponds to."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import json\n",
    "import trt_pose.coco\n",
    "\n",
    "with open('models/human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll load our model. Each model takes at least two parameters, cmap_channels and paf_channels corresponding to the number of heatmap channels and part affinity field channels. The number of part affinity field channels is 2x the number of links, because each link has a channel corresponding to the x and y direction of the vector field for each link.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import trt_pose.models\n",
    "\n",
    "num_parts = len(human_pose['keypoints'])\n",
    "num_links = len(human_pose['skeleton'])\n",
    "\n",
    "# model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()\n",
    "model = trt_pose.models.resnet50_baseline_att(num_parts, 2 * num_links).cuda().eval()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be8438cd688436d81a34742e2e4f254",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's load the model weights.  You will need to download these according to the table in the README."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import torch\n",
    "\n",
    "MODEL_WEIGHTS = 'models/resnet18_baseline_att_368x368_A_epoch_249.pth'\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to optimize with TensorRT using the python library *torch2trt* we'll also need to create some example data.  The dimensions\n",
    "of this data should match the dimensions that the network was trained with.  Since we're using the resnet18 variant that was trained on\n",
    "an input resolution of 224x224, we set the width and height to these dimensions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "WIDTH = 368\n",
    "HEIGHT = 368\n",
    "\n",
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll use [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) to optimize the model.  We'll enable fp16_mode to allow optimizations to use reduced half precision."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch2trt\n",
    "\n",
    "model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True, max_workspace_size=3<<30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "OPTIMIZED_MODEL = 'models/resnet18_baseline_att_368x368_A_epoch_249_trt.pth'\n",
    "\n",
    "torch.save(model_trt.state_dict(), OPTIMIZED_MODEL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}